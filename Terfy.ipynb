{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "history_visible": true,
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNtflgDKpO57k4HKrt9U3Fy",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/PersephoneKarnstein/terf-gen/blob/master/Terfy.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install keras_nlp\n",
        "!pip install nltk"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jbpRYpDoJ9T_",
        "outputId": "68185794-71ba-4e4e-dada-fe947a573562"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: keras_nlp in /usr/local/lib/python3.10/dist-packages (0.5.2)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from keras_nlp) (1.4.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from keras_nlp) (1.22.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from keras_nlp) (23.1)\n",
            "Requirement already satisfied: tensorflow-text in /usr/local/lib/python3.10/dist-packages (from keras_nlp) (2.12.1)\n",
            "Requirement already satisfied: tensorflow-hub>=0.8.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow-text->keras_nlp) (0.13.0)\n",
            "Requirement already satisfied: tensorflow<2.13,>=2.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow-text->keras_nlp) (2.12.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.13,>=2.12.0->tensorflow-text->keras_nlp) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.13,>=2.12.0->tensorflow-text->keras_nlp) (23.5.26)\n",
            "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.13,>=2.12.0->tensorflow-text->keras_nlp) (0.4.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.13,>=2.12.0->tensorflow-text->keras_nlp) (0.2.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.13,>=2.12.0->tensorflow-text->keras_nlp) (1.56.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.13,>=2.12.0->tensorflow-text->keras_nlp) (3.8.0)\n",
            "Requirement already satisfied: jax>=0.3.15 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.13,>=2.12.0->tensorflow-text->keras_nlp) (0.4.10)\n",
            "Requirement already satisfied: keras<2.13,>=2.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.13,>=2.12.0->tensorflow-text->keras_nlp) (2.12.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.13,>=2.12.0->tensorflow-text->keras_nlp) (16.0.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.13,>=2.12.0->tensorflow-text->keras_nlp) (3.3.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.13,>=2.12.0->tensorflow-text->keras_nlp) (3.20.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.13,>=2.12.0->tensorflow-text->keras_nlp) (67.7.2)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.13,>=2.12.0->tensorflow-text->keras_nlp) (1.16.0)\n",
            "Requirement already satisfied: tensorboard<2.13,>=2.12 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.13,>=2.12.0->tensorflow-text->keras_nlp) (2.12.3)\n",
            "Requirement already satisfied: tensorflow-estimator<2.13,>=2.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.13,>=2.12.0->tensorflow-text->keras_nlp) (2.12.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.13,>=2.12.0->tensorflow-text->keras_nlp) (2.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.13,>=2.12.0->tensorflow-text->keras_nlp) (4.6.3)\n",
            "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.13,>=2.12.0->tensorflow-text->keras_nlp) (1.14.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.13,>=2.12.0->tensorflow-text->keras_nlp) (0.32.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow<2.13,>=2.12.0->tensorflow-text->keras_nlp) (0.40.0)\n",
            "Requirement already satisfied: ml-dtypes>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from jax>=0.3.15->tensorflow<2.13,>=2.12.0->tensorflow-text->keras_nlp) (0.2.0)\n",
            "Requirement already satisfied: scipy>=1.7 in /usr/local/lib/python3.10/dist-packages (from jax>=0.3.15->tensorflow<2.13,>=2.12.0->tensorflow-text->keras_nlp) (1.10.1)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow<2.13,>=2.12.0->tensorflow-text->keras_nlp) (2.17.3)\n",
            "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow<2.13,>=2.12.0->tensorflow-text->keras_nlp) (1.0.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow<2.13,>=2.12.0->tensorflow-text->keras_nlp) (3.4.3)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow<2.13,>=2.12.0->tensorflow-text->keras_nlp) (2.27.1)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow<2.13,>=2.12.0->tensorflow-text->keras_nlp) (0.7.1)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow<2.13,>=2.12.0->tensorflow-text->keras_nlp) (2.3.6)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow<2.13,>=2.12.0->tensorflow-text->keras_nlp) (5.3.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow<2.13,>=2.12.0->tensorflow-text->keras_nlp) (0.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow<2.13,>=2.12.0->tensorflow-text->keras_nlp) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow<2.13,>=2.12.0->tensorflow-text->keras_nlp) (1.3.1)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow<2.13,>=2.12.0->tensorflow-text->keras_nlp) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow<2.13,>=2.12.0->tensorflow-text->keras_nlp) (2023.5.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow<2.13,>=2.12.0->tensorflow-text->keras_nlp) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow<2.13,>=2.12.0->tensorflow-text->keras_nlp) (3.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.13,>=2.12->tensorflow<2.13,>=2.12.0->tensorflow-text->keras_nlp) (2.1.3)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow<2.13,>=2.12.0->tensorflow-text->keras_nlp) (0.5.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow<2.13,>=2.12.0->tensorflow-text->keras_nlp) (3.2.2)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.3)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.2.0)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2022.10.31)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.65.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "E0iYOJjZJZ_1"
      },
      "outputs": [],
      "source": [
        "#https://stackabuse.com/gpt-style-text-generation-in-python-with-tensorflowkeras/\n",
        "\n",
        "import os, glob, keras_nlp, nltk.data, random,warnings\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from keras.models import model_from_json\n",
        "import numpy as np\n",
        "\n",
        "nltk.download('punkt', quiet=True)\n",
        "\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
        "warnings.filterwarnings(\"ignore\", category=UserWarning)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def get_corpus_data():\n",
        "\tpath = \"/content/sample_data\"\n",
        "\tfiles = glob.glob(path + '/*.txt')\n",
        "\tdata = \"\"\n",
        "\t# files = [files[1]] #delete this line, this is just for testing\n",
        "\tfor f in files:\n",
        "\t\tdata += open(f).read()\n",
        "\treturn data\n",
        "\n",
        "texts = get_corpus_data()\n"
      ],
      "metadata": {
        "id": "EUXHzQu4J2XZ"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# console.print(\"[pink1]Decimating...\")\n",
        "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
        "text_list = tokenizer.tokenize(texts)\n",
        "text_list = list(filter(None, text_list))\n",
        "\n",
        "random.shuffle(text_list)\n",
        "\n",
        "length = len(text_list)\n",
        "text_train = text_list[:int(0.7*length)]\n",
        "text_test = text_list[int(0.7*length):int(0.85*length)]\n",
        "text_valid = text_list[int(0.85*length):]\n"
      ],
      "metadata": {
        "id": "VFK0i7NIJ67w"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.layers import TextVectorization\n",
        "\n",
        "def custom_standardization(input_string):\n",
        "    sentence = tf.strings.lower(input_string)\n",
        "    sentence = tf.strings.regex_replace(sentence, \"\\n\", \" \")\n",
        "    return sentence\n"
      ],
      "metadata": {
        "id": "ft9KzQMcLmHA"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "maxlen = len(max(text_list))\n",
        "\n",
        "vectorize_layer = TextVectorization(\n",
        "    standardize = custom_standardization,\n",
        "    output_mode=\"int\",\n",
        "    output_sequence_length=maxlen + 1,\n",
        ")\n",
        "\n",
        "vectorize_layer.adapt(text_list)\n",
        "vocab = vectorize_layer.get_vocabulary()\n",
        "vocab_size = len(vocab)\n",
        "\n",
        "index_lookup = dict(zip(range(len(vocab)), vocab))\n",
        "# index_lookup[5]\n",
        "\n",
        "batch_size = 64\n"
      ],
      "metadata": {
        "id": "HfeLunPFLr0V"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices(text_train)\n",
        "train_dataset = train_dataset.shuffle(buffer_size=256)\n",
        "train_dataset = train_dataset.batch(batch_size)\n",
        "\n",
        "test_dataset = tf.data.Dataset.from_tensor_slices(text_test)\n",
        "test_dataset = test_dataset.shuffle(buffer_size=256)\n",
        "test_dataset = test_dataset.batch(batch_size)\n",
        "\n",
        "valid_dataset = tf.data.Dataset.from_tensor_slices(text_valid)\n",
        "valid_dataset = valid_dataset.shuffle(buffer_size=256)\n",
        "valid_dataset = valid_dataset.batch(batch_size)\n"
      ],
      "metadata": {
        "id": "VhepDYYeLwhh"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_text(text):\n",
        "    text = tf.expand_dims(text, -1)\n",
        "    tokenized_sentences = vectorize_layer(text)\n",
        "    x = tokenized_sentences[:, :-1]\n",
        "    y = tokenized_sentences[:, 1:]\n",
        "    return x, y\n",
        "\n"
      ],
      "metadata": {
        "id": "9gin1vMLLzlR"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = train_dataset.map(preprocess_text)\n",
        "train_dataset = train_dataset.prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "test_dataset = test_dataset.map(preprocess_text)\n",
        "test_dataset = test_dataset.prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "valid_dataset = valid_dataset.map(preprocess_text)\n",
        "valid_dataset = valid_dataset.prefetch(tf.data.AUTOTUNE)\n"
      ],
      "metadata": {
        "id": "ejyNZa_0L3Kf"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embed_dim = 128\n",
        "num_heads = 4\n",
        "\n",
        "def create_model():\n",
        "    inputs = keras.layers.Input(shape=(maxlen,), dtype=tf.int32)\n",
        "    x = keras_nlp.layers.TokenAndPositionEmbedding(vocab_size, maxlen, embed_dim)(inputs)\n",
        "    for i in range(4):\n",
        "        x = keras_nlp.layers.TransformerDecoder(intermediate_dim=embed_dim*2, num_heads=num_heads, dropout=0.5)(x)\n",
        "    do = keras.layers.Dropout(0.4)(x)\n",
        "    outputs = keras.layers.Dense(vocab_size, activation='softmax')(do)\n",
        "    model = keras.Model(inputs=inputs, outputs=outputs)\n",
        "    model.compile(\n",
        "        optimizer=\"adam\",\n",
        "        loss='sparse_categorical_crossentropy',\n",
        "        metrics=[keras_nlp.metrics.Perplexity(), 'accuracy']\n",
        "    )\n",
        "    return model\n"
      ],
      "metadata": {
        "id": "mYF5uZPfL7Vx"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "model = create_model()\n",
        "model.summary()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5J1zkhK3L-8a",
        "outputId": "3911291d-365d-44cd-92c4-e35a89e31099"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_1 (InputLayer)        [(None, 56)]              0         \n",
            "                                                                 \n",
            " token_and_position_embeddin  (None, 56, 128)          8482816   \n",
            " g (TokenAndPositionEmbeddin                                     \n",
            " g)                                                              \n",
            "                                                                 \n",
            " transformer_decoder (Transf  (None, 56, 128)          132480    \n",
            " ormerDecoder)                                                   \n",
            "                                                                 \n",
            " transformer_decoder_1 (Tran  (None, 56, 128)          132480    \n",
            " sformerDecoder)                                                 \n",
            "                                                                 \n",
            " transformer_decoder_2 (Tran  (None, 56, 128)          132480    \n",
            " sformerDecoder)                                                 \n",
            "                                                                 \n",
            " transformer_decoder_3 (Tran  (None, 56, 128)          132480    \n",
            " sformerDecoder)                                                 \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 56, 128)           0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 56, 66216)         8541864   \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 17,554,600\n",
            "Trainable params: 17,554,600\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class TextSampler(keras.callbacks.Callback):\n",
        "    def __init__(self, start_prompt, max_tokens):\n",
        "        self.start_prompt = start_prompt\n",
        "        self.max_tokens = max_tokens\n",
        "    def sample_token(self, logits):\n",
        "        logits, indices = tf.math.top_k(logits, k=5, sorted=True)\n",
        "        indices = np.asarray(indices).astype(\"int32\")\n",
        "        preds = keras.activations.softmax(tf.expand_dims(logits, 0))[0]\n",
        "        preds = np.asarray(preds).astype(\"float32\")\n",
        "        return np.random.choice(indices, p=preds)\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        decoded_sample = self.start_prompt\n",
        "        for i in range(self.max_tokens-1):\n",
        "            tokenized_prompt = vectorize_layer([decoded_sample])[:, :-1]\n",
        "            predictions = self.model.predict([tokenized_prompt], verbose=0)\n",
        "            sample_index = len(decoded_sample.strip().split())-1\n",
        "            sampled_token = self.sample_token(predictions[0][sample_index])\n",
        "            sampled_token = index_lookup[sampled_token]\n",
        "            decoded_sample += \" \" + sampled_token\n",
        "        print(f\"\\nSample text:\\n{decoded_sample}...\\n\")\n"
      ],
      "metadata": {
        "id": "9Qe2MYkpMDlf"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# First 5 words of a random sentence to be used as a seed\n",
        "random_sentence = ' '.join(random.choice(text_valid).replace('\\n', ' ').split(' ')[:4])\n",
        "sampler = TextSampler(random_sentence, 30)\n",
        "reducelr = keras.callbacks.ReduceLROnPlateau(patience=10, monitor='val_loss')\n",
        "\n"
      ],
      "metadata": {
        "id": "uAd0N85jMIiT"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def sample_token(logits):\n",
        "        logits, indices = tf.math.top_k(logits, k=5, sorted=True)\n",
        "        indices = np.asarray(indices).astype(\"int32\")\n",
        "        preds = keras.activations.softmax(tf.expand_dims(logits, 0))[0]\n",
        "        preds = np.asarray(preds).astype(\"float32\")\n",
        "        return np.random.choice(indices, p=preds)\n",
        "\n",
        "def generate_text(prompt, response_length=20):\n",
        "    decoded_sample = prompt\n",
        "    for i in range(response_length-1):\n",
        "        tokenized_prompt = vectorize_layer([decoded_sample])[:, :-1]\n",
        "        predictions = model.predict([tokenized_prompt], verbose=0)\n",
        "        sample_index = len(decoded_sample.strip().split())-1\n",
        "        sampled_token = sample_token(predictions[0][sample_index])\n",
        "        sampled_token = index_lookup[sampled_token]\n",
        "        decoded_sample += \" \" + sampled_token\n",
        "    return decoded_sample\n",
        "\n",
        "def save_model(model):\n",
        "\t# serialize model to JSON\n",
        "\tmodel_json = model.to_json()\n",
        "\twith open(\"/content/models/model.json\", \"w\") as json_file:\n",
        "\t\tjson_file.write(model_json)\n",
        "\t# serialize weights to HDF5\n",
        "\tmodel.save_weights(\"/content/models/model.h5\")\n",
        "\t# print(\"Saved model to disk\")\n",
        "\n",
        "def load_model():\n",
        "    path = os.getcwd()\n",
        "    # with redirect_stdout(open(os.devnull, 'w')):\n",
        "    json_file = open(\"/content/models/model.json\", 'r')\n",
        "    loaded_model_json = json_file.read()\n",
        "    json_file.close()\n",
        "    loaded_model = model_from_json(loaded_model_json)\n",
        "    # load weights into new model\n",
        "    loaded_model.load_weights(\"/content/models/model.h5\")\n",
        "    loaded_model.compile(\n",
        "        optimizer=\"adam\",\n",
        "        loss='sparse_categorical_crossentropy',\n",
        "        metrics=[keras_nlp.metrics.Perplexity(), 'accuracy'])\n",
        "    print(\"Loaded model from disk\")\n",
        "    return loaded_model\n"
      ],
      "metadata": {
        "id": "TsgpDSXRMMRj"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "model = create_model()\n",
        "history = model.fit(train_dataset,\n",
        "                    validation_data=valid_dataset,\n",
        "                    epochs=50,\n",
        "                    callbacks=[sampler, reducelr])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tZjZgMxGMQeB",
        "outputId": "9c3558a4-2c8f-4f2b-b12a-8d1fda1569c8"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "324/324 [==============================] - ETA: 0s - loss: 3.6659 - perplexity: 39.0905 - accuracy: 0.6259\n",
            "Sample text:\n",
            "It practically writes itself.  to a same woman   in the lot  to a same same same the lot and be be woman to same be have a woman and...\n",
            "\n",
            "324/324 [==============================] - 144s 389ms/step - loss: 3.6659 - perplexity: 39.0905 - accuracy: 0.6259 - val_loss: 2.8693 - val_perplexity: 17.6249 - val_accuracy: 0.6371 - lr: 0.0010\n",
            "Epoch 2/50\n",
            "324/324 [==============================] - ETA: 0s - loss: 2.7707 - perplexity: 15.9697 - accuracy: 0.6472\n",
            "Sample text:\n",
            "It practically writes itself.   to be not been a own man in the woman of the lot  of their woman is a woman and the same man is a man...\n",
            "\n",
            "324/324 [==============================] - 112s 345ms/step - loss: 2.7707 - perplexity: 15.9697 - accuracy: 0.6472 - val_loss: 2.7332 - val_perplexity: 15.3828 - val_accuracy: 0.6539 - lr: 0.0010\n",
            "Epoch 3/50\n",
            "324/324 [==============================] - ETA: 0s - loss: 2.5996 - perplexity: 13.4590 - accuracy: 0.6631\n",
            "Sample text:\n",
            "It practically writes itself. to have a same or an sex reassignment as if it is that the same is the woman and i had a few and i think that i had...\n",
            "\n",
            "324/324 [==============================] - 102s 315ms/step - loss: 2.5996 - perplexity: 13.4590 - accuracy: 0.6631 - val_loss: 2.6619 - val_perplexity: 14.3240 - val_accuracy: 0.6636 - lr: 0.0010\n",
            "Epoch 4/50\n",
            "324/324 [==============================] - ETA: 0s - loss: 2.4597 - perplexity: 11.7013 - accuracy: 0.6727\n",
            "Sample text:\n",
            "It practically writes itself. and i think that i was not just just like you are a few or in the same or not a lot and a woman who were so that...\n",
            "\n",
            "324/324 [==============================] - 101s 312ms/step - loss: 2.4597 - perplexity: 11.7013 - accuracy: 0.6727 - val_loss: 2.6240 - val_perplexity: 13.7911 - val_accuracy: 0.6683 - lr: 0.0010\n",
            "Epoch 5/50\n",
            "324/324 [==============================] - ETA: 0s - loss: 2.3292 - perplexity: 10.2692 - accuracy: 0.6802\n",
            "Sample text:\n",
            "It practically writes itself. and i have a few months on a lot to the same people are a woman.  for the most of my body is the same people who had...\n",
            "\n",
            "324/324 [==============================] - 100s 309ms/step - loss: 2.3292 - perplexity: 10.2692 - accuracy: 0.6802 - val_loss: 2.5975 - val_perplexity: 13.4302 - val_accuracy: 0.6709 - lr: 0.0010\n",
            "Epoch 6/50\n",
            "324/324 [==============================] - ETA: 0s - loss: 2.2064 - perplexity: 9.0834 - accuracy: 0.6867\n",
            "Sample text:\n",
            "It practically writes itself.  ) , in this has always a new wave feminism was in the transsexual empire.   and a new zealand in a man is a few months...\n",
            "\n",
            "324/324 [==============================] - 99s 307ms/step - loss: 2.2064 - perplexity: 9.0834 - accuracy: 0.6867 - val_loss: 2.5989 - val_perplexity: 13.4490 - val_accuracy: 0.6727 - lr: 0.0010\n",
            "Epoch 7/50\n",
            "324/324 [==============================] - ETA: 0s - loss: 2.0882 - perplexity: 8.0702 - accuracy: 0.6950\n",
            "Sample text:\n",
            "It practically writes itself. ( 2009 )  . ; raymond, 1999 in the first is not to the same thing in my mother’s clothes, of his own body and his mother’s clothing...\n",
            "\n",
            "324/324 [==============================] - 99s 307ms/step - loss: 2.0882 - perplexity: 8.0702 - accuracy: 0.6950 - val_loss: 2.6215 - val_perplexity: 13.7559 - val_accuracy: 0.6730 - lr: 0.0010\n",
            "Epoch 8/50\n",
            "324/324 [==============================] - ETA: 0s - loss: 1.9787 - perplexity: 7.2334 - accuracy: 0.7030\n",
            "Sample text:\n",
            "It practically writes itself.  is the process and i was not yet i have been a woman, but the same way of my mother’s closet in this way. for some people and...\n",
            "\n",
            "324/324 [==============================] - 106s 326ms/step - loss: 1.9787 - perplexity: 7.2334 - accuracy: 0.7030 - val_loss: 2.6486 - val_perplexity: 14.1345 - val_accuracy: 0.6739 - lr: 0.0010\n",
            "Epoch 9/50\n",
            "323/324 [============================>.] - ETA: 0s - loss: 1.8823 - perplexity: 6.5683 - accuracy: 0.7095\n",
            "Sample text:\n",
            "It practically writes itself.   and this sub and has to the first cause of this process of course, in the current sociocultural of the problem of their husbands’ behavior and incomprehension...\n",
            "\n",
            "324/324 [==============================] - 100s 307ms/step - loss: 1.8822 - perplexity: 6.5679 - accuracy: 0.7095 - val_loss: 2.6943 - val_perplexity: 14.7948 - val_accuracy: 0.6733 - lr: 0.0010\n",
            "Epoch 10/50\n",
            "323/324 [============================>.] - ETA: 0s - loss: 1.8016 - perplexity: 6.0593 - accuracy: 0.7148\n",
            "Sample text:\n",
            "It practically writes itself. in a lot of this is the fact of your own skin.  and you were just a good reason if they were more likely to go on the...\n",
            "\n",
            "324/324 [==============================] - 99s 305ms/step - loss: 1.8016 - perplexity: 6.0591 - accuracy: 0.7148 - val_loss: 2.7380 - val_perplexity: 15.4562 - val_accuracy: 0.6728 - lr: 0.0010\n",
            "Epoch 11/50\n",
            "324/324 [==============================] - ETA: 0s - loss: 1.7366 - perplexity: 5.6782 - accuracy: 0.7192\n",
            "Sample text:\n",
            "It practically writes itself. ) . '  ) i don’t need to have some cases in fact, it seems like the best friend, and even if it was not to do not...\n",
            "\n",
            "324/324 [==============================] - 99s 305ms/step - loss: 1.7366 - perplexity: 5.6782 - accuracy: 0.7192 - val_loss: 2.7701 - val_perplexity: 15.9599 - val_accuracy: 0.6709 - lr: 0.0010\n",
            "Epoch 12/50\n",
            "324/324 [==============================] - ETA: 0s - loss: 1.6867 - perplexity: 5.4019 - accuracy: 0.7223\n",
            "Sample text:\n",
            "It practically writes itself.  ). ' for this story p.s. and    the fact that i don't want to  do not have the rest that  it may also...\n",
            "\n",
            "324/324 [==============================] - 99s 306ms/step - loss: 1.6867 - perplexity: 5.4019 - accuracy: 0.7223 - val_loss: 2.8138 - val_perplexity: 16.6724 - val_accuracy: 0.6715 - lr: 0.0010\n",
            "Epoch 13/50\n",
            "323/324 [============================>.] - ETA: 0s - loss: 1.6320 - perplexity: 5.1143 - accuracy: 0.7268\n",
            "Sample text:\n",
            "It practically writes itself.  when i have a problem to see how they can have to do you need to be done.i protect the next chapter, whereas we are just like the...\n",
            "\n",
            "324/324 [==============================] - 98s 303ms/step - loss: 1.6320 - perplexity: 5.1142 - accuracy: 0.7268 - val_loss: 2.8488 - val_perplexity: 17.2675 - val_accuracy: 0.6705 - lr: 0.0010\n",
            "Epoch 14/50\n",
            "323/324 [============================>.] - ETA: 0s - loss: 1.5941 - perplexity: 4.9240 - accuracy: 0.7303\n",
            "Sample text:\n",
            "It practically writes itself.  and i'm sure this issue.families that the same thing a man who hate my womb.  and this story was a woman.  ) .  ) ...\n",
            "\n",
            "324/324 [==============================] - 105s 325ms/step - loss: 1.5941 - perplexity: 4.9238 - accuracy: 0.7303 - val_loss: 2.8853 - val_perplexity: 17.9089 - val_accuracy: 0.6700 - lr: 0.0010\n",
            "Epoch 15/50\n",
            "324/324 [==============================] - ETA: 0s - loss: 1.5548 - perplexity: 4.7342 - accuracy: 0.7338\n",
            "Sample text:\n",
            "It practically writes itself. and this is not the united states. when he went from her paper, 10% of my mother’s calm. ' )  study and‘therapy’ that it should not have no...\n",
            "\n",
            "324/324 [==============================] - 98s 303ms/step - loss: 1.5548 - perplexity: 4.7342 - accuracy: 0.7338 - val_loss: 2.9211 - val_perplexity: 18.5614 - val_accuracy: 0.6692 - lr: 0.0010\n",
            "Epoch 16/50\n",
            "324/324 [==============================] - ETA: 0s - loss: 1.4624 - perplexity: 4.3165 - accuracy: 0.7437\n",
            "Sample text:\n",
            "It practically writes itself. ' )  , the same way, although the fact it has nothing but the same thing that the idea of my sexual harassment or not. and basketball. )....\n",
            "\n",
            "324/324 [==============================] - 98s 303ms/step - loss: 1.4624 - perplexity: 4.3165 - accuracy: 0.7437 - val_loss: 2.9325 - val_perplexity: 18.7751 - val_accuracy: 0.6707 - lr: 1.0000e-04\n",
            "Epoch 17/50\n",
            "323/324 [============================>.] - ETA: 0s - loss: 1.4373 - perplexity: 4.2092 - accuracy: 0.7468\n",
            "Sample text:\n",
            "It practically writes itself. ) study in some protest, it's a girl, but that she is not a few episodes of a few years of a kid, i don't have sown can never...\n",
            "\n",
            "324/324 [==============================] - 105s 325ms/step - loss: 1.4372 - perplexity: 4.2090 - accuracy: 0.7468 - val_loss: 2.9407 - val_perplexity: 18.9297 - val_accuracy: 0.6707 - lr: 1.0000e-04\n",
            "Epoch 18/50\n",
            "324/324 [==============================] - ETA: 0s - loss: 1.4210 - perplexity: 4.1412 - accuracy: 0.7487\n",
            "Sample text:\n",
            "It practically writes itself.  ) , which he does seem primi-tive by sex—gay men are very young people don't want you can be the fact that we have been arrested or rape.https://www.youtube.com/watch?v=_ziqdrfefbk>...\n",
            "\n",
            "324/324 [==============================] - 98s 304ms/step - loss: 1.4210 - perplexity: 4.1412 - accuracy: 0.7487 - val_loss: 2.9522 - val_perplexity: 19.1485 - val_accuracy: 0.6705 - lr: 1.0000e-04\n",
            "Epoch 19/50\n",
            "324/324 [==============================] - ETA: 0s - loss: 1.4075 - perplexity: 4.0856 - accuracy: 0.7504\n",
            "Sample text:\n",
            "It practically writes itself.  ). study and this sub is no matter for this book and offer lessons for my mon-strous tranny-cock embodies womanhood less than this way they were dateable transgender...\n",
            "\n",
            "324/324 [==============================] - 98s 303ms/step - loss: 1.4075 - perplexity: 4.0856 - accuracy: 0.7504 - val_loss: 2.9678 - val_perplexity: 19.4500 - val_accuracy: 0.6706 - lr: 1.0000e-04\n",
            "Epoch 20/50\n",
            "324/324 [==============================] - ETA: 0s - loss: 1.3979 - perplexity: 4.0465 - accuracy: 0.7516\n",
            "Sample text:\n",
            "It practically writes itself. '  ). ' : 144).  ) and the idea what i am pretty good and the idea that it would have the past, and the next year...\n",
            "\n",
            "324/324 [==============================] - 105s 326ms/step - loss: 1.3979 - perplexity: 4.0465 - accuracy: 0.7516 - val_loss: 2.9775 - val_perplexity: 19.6377 - val_accuracy: 0.6704 - lr: 1.0000e-04\n",
            "Epoch 21/50\n",
            "324/324 [==============================] - ETA: 0s - loss: 1.3865 - perplexity: 4.0009 - accuracy: 0.7529\n",
            "Sample text:\n",
            "It practically writes itself. when it is not a lot for example, he is still not a bunch of course in general, and regardless, she can tell her husband.   ) ...\n",
            "\n",
            "324/324 [==============================] - 99s 306ms/step - loss: 1.3865 - perplexity: 4.0009 - accuracy: 0.7529 - val_loss: 2.9802 - val_perplexity: 19.6921 - val_accuracy: 0.6700 - lr: 1.0000e-04\n",
            "Epoch 22/50\n",
            "323/324 [============================>.] - ETA: 0s - loss: 1.3780 - perplexity: 3.9671 - accuracy: 0.7540\n",
            "Sample text:\n",
            "It practically writes itself. )  . study but if you are a big pharma, because we have some other people don't know if the next generation are a child on this kind...\n",
            "\n",
            "324/324 [==============================] - 99s 304ms/step - loss: 1.3781 - perplexity: 3.9673 - accuracy: 0.7540 - val_loss: 2.9908 - val_perplexity: 19.9021 - val_accuracy: 0.6700 - lr: 1.0000e-04\n",
            "Epoch 23/50\n",
            "323/324 [============================>.] - ETA: 0s - loss: 1.3691 - perplexity: 3.9318 - accuracy: 0.7550\n",
            "Sample text:\n",
            "It practically writes itself. when he was not yet the first of her husband and tense.  and you can get some point out of these individuals.  ). when it seems for...\n",
            "\n",
            "324/324 [==============================] - 99s 305ms/step - loss: 1.3691 - perplexity: 3.9319 - accuracy: 0.7550 - val_loss: 3.0013 - val_perplexity: 20.1122 - val_accuracy: 0.6696 - lr: 1.0000e-04\n",
            "Epoch 24/50\n",
            "324/324 [==============================] - ETA: 0s - loss: 1.3616 - perplexity: 3.9023 - accuracy: 0.7561\n",
            "Sample text:\n",
            "It practically writes itself. and this is not even the whole \"conspiracy thing\" talking about: the last week after the same time, january 11, he has been affirming, the school boards in bristol,...\n",
            "\n",
            "324/324 [==============================] - 98s 303ms/step - loss: 1.3616 - perplexity: 3.9023 - accuracy: 0.7561 - val_loss: 3.0101 - val_perplexity: 20.2891 - val_accuracy: 0.6696 - lr: 1.0000e-04\n",
            "Epoch 25/50\n",
            "323/324 [============================>.] - ETA: 0s - loss: 1.3538 - perplexity: 3.8721 - accuracy: 0.7570\n",
            "Sample text:\n",
            "It practically writes itself.  )  , i am a huge percentage of people are very usefulyup.  and this story is a new world ofqueer gender dysphoria.    study...\n",
            "\n",
            "324/324 [==============================] - 98s 303ms/step - loss: 1.3538 - perplexity: 3.8722 - accuracy: 0.7570 - val_loss: 3.0175 - val_perplexity: 20.4406 - val_accuracy: 0.6693 - lr: 1.0000e-04\n",
            "Epoch 26/50\n",
            "324/324 [==============================] - ETA: 0s - loss: 1.3467 - perplexity: 3.8446 - accuracy: 0.7578\n",
            "Sample text:\n",
            "It practically writes itself. california boasts for the vast expertise is outmoded from this chapter.  paper of this manuscript and the camps are not dispensed with, or a problem and me”): ”...\n",
            "\n",
            "324/324 [==============================] - 99s 305ms/step - loss: 1.3467 - perplexity: 3.8446 - accuracy: 0.7578 - val_loss: 3.0139 - val_perplexity: 20.3675 - val_accuracy: 0.6697 - lr: 1.0000e-05\n",
            "Epoch 27/50\n",
            "323/324 [============================>.] - ETA: 0s - loss: 1.3434 - perplexity: 3.8321 - accuracy: 0.7581\n",
            "Sample text:\n",
            "It practically writes itself. )  . ) decided to look in some ways he can never be able to try in my opinion, but she had a man and the fact that...\n",
            "\n",
            "324/324 [==============================] - 98s 304ms/step - loss: 1.3434 - perplexity: 3.8320 - accuracy: 0.7581 - val_loss: 3.0148 - val_perplexity: 20.3848 - val_accuracy: 0.6697 - lr: 1.0000e-05\n",
            "Epoch 28/50\n",
            "324/324 [==============================] - ETA: 0s - loss: 1.3414 - perplexity: 3.8245 - accuracy: 0.7584\n",
            "Sample text:\n",
            "It practically writes itself.   and jessica yanov was one in this new enemy?  )  , which we were chosen, and mimic the subject, disagrees with this respect. article on...\n",
            "\n",
            "324/324 [==============================] - 98s 303ms/step - loss: 1.3414 - perplexity: 3.8245 - accuracy: 0.7584 - val_loss: 3.0145 - val_perplexity: 20.3786 - val_accuracy: 0.6697 - lr: 1.0000e-05\n",
            "Epoch 29/50\n",
            "323/324 [============================>.] - ETA: 0s - loss: 1.3408 - perplexity: 3.8221 - accuracy: 0.7583\n",
            "Sample text:\n",
            "It practically writes itself.  ) . ) decided to the first step and this insight. study researched a lot of my opinion, it's not sure if they don't know if the same...\n",
            "\n",
            "324/324 [==============================] - 98s 302ms/step - loss: 1.3408 - perplexity: 3.8220 - accuracy: 0.7583 - val_loss: 3.0150 - val_perplexity: 20.3888 - val_accuracy: 0.6697 - lr: 1.0000e-05\n",
            "Epoch 30/50\n",
            "323/324 [============================>.] - ETA: 0s - loss: 1.3401 - perplexity: 3.8196 - accuracy: 0.7586\n",
            "Sample text:\n",
            "It practically writes itself. ' study in my eyes widened but he could hardly been a couple years old. ) and this area and then one year old. ) .  study of...\n",
            "\n",
            "324/324 [==============================] - 105s 324ms/step - loss: 1.3402 - perplexity: 3.8196 - accuracy: 0.7586 - val_loss: 3.0165 - val_perplexity: 20.4196 - val_accuracy: 0.6697 - lr: 1.0000e-05\n",
            "Epoch 31/50\n",
            "324/324 [==============================] - ETA: 0s - loss: 1.3388 - perplexity: 3.8145 - accuracy: 0.7588\n",
            "Sample text:\n",
            "It practically writes itself. and jessica yaniv work, it was the idea for some of the same sex, then there were concernedto construct a diagnosis informs for fucks sake the mostsignifi am at...\n",
            "\n",
            "324/324 [==============================] - 98s 303ms/step - loss: 1.3388 - perplexity: 3.8145 - accuracy: 0.7588 - val_loss: 3.0179 - val_perplexity: 20.4482 - val_accuracy: 0.6697 - lr: 1.0000e-05\n",
            "Epoch 32/50\n",
            "324/324 [==============================] - ETA: 0s - loss: 1.3370 - perplexity: 3.8075 - accuracy: 0.7587\n",
            "Sample text:\n",
            "It practically writes itself.     '    ).  ) , the last election.  work at this book, the father  and then he was banned fromspeaking...\n",
            "\n",
            "324/324 [==============================] - 98s 302ms/step - loss: 1.3370 - perplexity: 3.8075 - accuracy: 0.7587 - val_loss: 3.0187 - val_perplexity: 20.4640 - val_accuracy: 0.6696 - lr: 1.0000e-05\n",
            "Epoch 33/50\n",
            "324/324 [==============================] - ETA: 0s - loss: 1.3351 - perplexity: 3.8005 - accuracy: 0.7588\n",
            "Sample text:\n",
            "It practically writes itself. california education in this book and cohen-kettenis 2015, 20, fails, which causes unhappiness and reconstructive surgeons connected with the concept in my own interview as well.  ' study...\n",
            "\n",
            "324/324 [==============================] - 99s 305ms/step - loss: 1.3351 - perplexity: 3.8005 - accuracy: 0.7588 - val_loss: 3.0191 - val_perplexity: 20.4726 - val_accuracy: 0.6696 - lr: 1.0000e-05\n",
            "Epoch 34/50\n",
            "324/324 [==============================] - ETA: 0s - loss: 1.3350 - perplexity: 3.7998 - accuracy: 0.7590\n",
            "Sample text:\n",
            "It practically writes itself. ' study when we were the first step,” van deven, “how mental illness has a giant party, or this book 1984 )  .some informant described the late twenties,...\n",
            "\n",
            "324/324 [==============================] - 105s 326ms/step - loss: 1.3350 - perplexity: 3.7998 - accuracy: 0.7590 - val_loss: 3.0198 - val_perplexity: 20.4877 - val_accuracy: 0.6696 - lr: 1.0000e-05\n",
            "Epoch 35/50\n",
            "323/324 [============================>.] - ETA: 0s - loss: 1.3336 - perplexity: 3.7948 - accuracy: 0.7592\n",
            "Sample text:\n",
            "It practically writes itself.  ) .  ' ) , i don't think it's not to the father for a lot about levay’s study was a study researched in fact, in my...\n",
            "\n",
            "324/324 [==============================] - 98s 302ms/step - loss: 1.3336 - perplexity: 3.7946 - accuracy: 0.7592 - val_loss: 3.0209 - val_perplexity: 20.5091 - val_accuracy: 0.6695 - lr: 1.0000e-05\n",
            "Epoch 36/50\n",
            "324/324 [==============================] - ETA: 0s - loss: 1.3331 - perplexity: 3.7926 - accuracy: 0.7595\n",
            "Sample text:\n",
            "It practically writes itself. ' ). ) . comment on reddit suspects that these data are labelled‘cisgender’, and this in some people are imprisoned for the slack, these medical/surgical boundaries.   ...\n",
            "\n",
            "324/324 [==============================] - 99s 304ms/step - loss: 1.3331 - perplexity: 3.7926 - accuracy: 0.7595 - val_loss: 3.0199 - val_perplexity: 20.4889 - val_accuracy: 0.6696 - lr: 1.0000e-06\n",
            "Epoch 37/50\n",
            "324/324 [==============================] - ETA: 0s - loss: 1.3322 - perplexity: 3.7894 - accuracy: 0.7598\n",
            "Sample text:\n",
            "It practically writes itself.   ' ' term isused parlors or be a few weeks and have been dressed as a lot of this sub and even the fact it would love...\n",
            "\n",
            "324/324 [==============================] - 98s 304ms/step - loss: 1.3322 - perplexity: 3.7894 - accuracy: 0.7598 - val_loss: 3.0194 - val_perplexity: 20.4787 - val_accuracy: 0.6696 - lr: 1.0000e-06\n",
            "Epoch 38/50\n",
            "324/324 [==============================] - ETA: 0s - loss: 1.3318 - perplexity: 3.7879 - accuracy: 0.7596\n",
            "Sample text:\n",
            "It practically writes itself.  and cohen-kettenis ( 1998 ) study reportedhistories that feminine than by sexual attraction and heterosexual male sexual attraction and sexual fantasies are unable in some autogynephilic sexual orientation...\n",
            "\n",
            "324/324 [==============================] - 98s 303ms/step - loss: 1.3318 - perplexity: 3.7879 - accuracy: 0.7596 - val_loss: 3.0189 - val_perplexity: 20.4681 - val_accuracy: 0.6696 - lr: 1.0000e-06\n",
            "Epoch 39/50\n",
            "323/324 [============================>.] - ETA: 0s - loss: 1.3311 - perplexity: 3.7853 - accuracy: 0.7596\n",
            "Sample text:\n",
            "It practically writes itself. and i am a study and this story is the united states, and psychiatrists and psychologic-ally devastating that i don't want the exception for this guy’s a scornful tone,...\n",
            "\n",
            "324/324 [==============================] - 98s 302ms/step - loss: 1.3311 - perplexity: 3.7851 - accuracy: 0.7596 - val_loss: 3.0187 - val_perplexity: 20.4644 - val_accuracy: 0.6696 - lr: 1.0000e-06\n",
            "Epoch 40/50\n",
            "323/324 [============================>.] - ETA: 0s - loss: 1.3307 - perplexity: 3.7837 - accuracy: 0.7595\n",
            "Sample text:\n",
            "It practically writes itself. '   ).   term for some people who were negative.its not a battalion that people are two centuries later, it just like it out. ' )....\n",
            "\n",
            "324/324 [==============================] - 98s 302ms/step - loss: 1.3307 - perplexity: 3.7836 - accuracy: 0.7596 - val_loss: 3.0186 - val_perplexity: 20.4622 - val_accuracy: 0.6696 - lr: 1.0000e-06\n",
            "Epoch 41/50\n",
            "323/324 [============================>.] - ETA: 0s - loss: 1.3316 - perplexity: 3.7872 - accuracy: 0.7597\n",
            "Sample text:\n",
            "It practically writes itself.   ) .  ' )  study where he was the fact that some other people are so i guess what? ) , and i don't have...\n",
            "\n",
            "324/324 [==============================] - 98s 303ms/step - loss: 1.3317 - perplexity: 3.7874 - accuracy: 0.7596 - val_loss: 3.0187 - val_perplexity: 20.4645 - val_accuracy: 0.6696 - lr: 1.0000e-06\n",
            "Epoch 42/50\n",
            "324/324 [==============================] - ETA: 0s - loss: 1.3311 - perplexity: 3.7852 - accuracy: 0.7598\n",
            "Sample text:\n",
            "It practically writes itself. california boasts for the vast journalist theme, uniting of them starve? and narrowing of this book jacket or blindness.  study and‘therapy’ of a minute.” their gender nonconforming, gender...\n",
            "\n",
            "324/324 [==============================] - 98s 302ms/step - loss: 1.3311 - perplexity: 3.7852 - accuracy: 0.7598 - val_loss: 3.0186 - val_perplexity: 20.4625 - val_accuracy: 0.6696 - lr: 1.0000e-06\n",
            "Epoch 43/50\n",
            "323/324 [============================>.] - ETA: 0s - loss: 1.3309 - perplexity: 3.7843 - accuracy: 0.7599\n",
            "Sample text:\n",
            "It practically writes itself.  '  ).   ) and i am at some cases in the whole objectified nude woman, and some people and this dire prognosis, many young girls...\n",
            "\n",
            "324/324 [==============================] - 98s 302ms/step - loss: 1.3309 - perplexity: 3.7845 - accuracy: 0.7599 - val_loss: 3.0186 - val_perplexity: 20.4617 - val_accuracy: 0.6696 - lr: 1.0000e-06\n",
            "Epoch 44/50\n",
            "323/324 [============================>.] - ETA: 0s - loss: 1.3303 - perplexity: 3.7821 - accuracy: 0.7597\n",
            "Sample text:\n",
            "It practically writes itself.    california teachers can see this book and conversion?    ) , which we are not only thing he is theother dubious point; i.e. ...\n",
            "\n",
            "324/324 [==============================] - 105s 324ms/step - loss: 1.3302 - perplexity: 3.7819 - accuracy: 0.7597 - val_loss: 3.0188 - val_perplexity: 20.4659 - val_accuracy: 0.6696 - lr: 1.0000e-06\n",
            "Epoch 45/50\n",
            "323/324 [============================>.] - ETA: 0s - loss: 1.3311 - perplexity: 3.7850 - accuracy: 0.7596\n",
            "Sample text:\n",
            "It practically writes itself. ' ). study of these days before this subject. ), and the fact you know what they were unworthy and cruel to combine the fact that i had been...\n",
            "\n",
            "324/324 [==============================] - 98s 302ms/step - loss: 1.3310 - perplexity: 3.7848 - accuracy: 0.7596 - val_loss: 3.0188 - val_perplexity: 20.4672 - val_accuracy: 0.6696 - lr: 1.0000e-06\n",
            "Epoch 46/50\n",
            "324/324 [==============================] - ETA: 0s - loss: 1.3304 - perplexity: 3.7825 - accuracy: 0.7597\n",
            "Sample text:\n",
            "It practically writes itself. ' '  ' ' ' ) , a study the first sample size that he had been slightly different.the term gender identity clinics, are different than the idea...\n",
            "\n",
            "324/324 [==============================] - 98s 302ms/step - loss: 1.3304 - perplexity: 3.7825 - accuracy: 0.7597 - val_loss: 3.0188 - val_perplexity: 20.4675 - val_accuracy: 0.6696 - lr: 1.0000e-07\n",
            "Epoch 47/50\n",
            "323/324 [============================>.] - ETA: 0s - loss: 1.3310 - perplexity: 3.7849 - accuracy: 0.7595\n",
            "Sample text:\n",
            "It practically writes itself. ' study of teenage girls, “i can’t tell her parents who can’t seemto have been a few months it a huge club for a boy who decided to the...\n",
            "\n",
            "324/324 [==============================] - 105s 324ms/step - loss: 1.3310 - perplexity: 3.7848 - accuracy: 0.7595 - val_loss: 3.0188 - val_perplexity: 20.4675 - val_accuracy: 0.6696 - lr: 1.0000e-07\n",
            "Epoch 48/50\n",
            "323/324 [============================>.] - ETA: 0s - loss: 1.3297 - perplexity: 3.7801 - accuracy: 0.7598\n",
            "Sample text:\n",
            "It practically writes itself. and this subreddit and mumble that they are \"vapid\" so they stopped. paper, rules for drag strip the idea of sterilisationthrough june, its message, because we can see that...\n",
            "\n",
            "324/324 [==============================] - 98s 302ms/step - loss: 1.3297 - perplexity: 3.7799 - accuracy: 0.7598 - val_loss: 3.0188 - val_perplexity: 20.4673 - val_accuracy: 0.6696 - lr: 1.0000e-07\n",
            "Epoch 49/50\n",
            "324/324 [==============================] - ETA: 0s - loss: 1.3304 - perplexity: 3.7826 - accuracy: 0.7597\n",
            "Sample text:\n",
            "It practically writes itself.  ) . ) decided for these kids and i think they were the fact we are some ways chase rushes to revealwhat it is instructive.   ...\n",
            "\n",
            "324/324 [==============================] - 98s 303ms/step - loss: 1.3304 - perplexity: 3.7826 - accuracy: 0.7597 - val_loss: 3.0188 - val_perplexity: 20.4670 - val_accuracy: 0.6696 - lr: 1.0000e-07\n",
            "Epoch 50/50\n",
            "324/324 [==============================] - ETA: 0s - loss: 1.3314 - perplexity: 3.7862 - accuracy: 0.7595\n",
            "Sample text:\n",
            "It practically writes itself.  ' ) , p. 142).  : 503). ) study and‘therapy’ of a lot in a giant party, of this sub or the uk, this respect.postmodern and this...\n",
            "\n",
            "324/324 [==============================] - 98s 303ms/step - loss: 1.3314 - perplexity: 3.7862 - accuracy: 0.7595 - val_loss: 3.0188 - val_perplexity: 20.4676 - val_accuracy: 0.6696 - lr: 1.0000e-07\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "save_model(model)"
      ],
      "metadata": {
        "id": "ivN9bySfkg8N"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generate_text('the person I was when')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "DxLK1jrFkzXo",
        "outputId": "7de0d402-5e13-4425-9fd6-bbd71e00f3bc"
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'the person I was when the same way that i am at a girl who went to a girl at the time. : 4).'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 74
        }
      ]
    }
  ]
}